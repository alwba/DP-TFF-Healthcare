{"cells":[{"cell_type":"markdown","metadata":{"id":"ge-cLlDdVmbr"},"source":["# Notebook for federated training on diabetes dataset"]},{"cell_type":"markdown","metadata":{"id":"aDDMLTdkOslV"},"source":["## Installation of required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znPpaE8EtAHx"},"outputs":[],"source":["!pip install --quiet --upgrade dp-accounting==0.4.3\n","!pip install --quiet --upgrade tensorflow-federated==0.84.0"]},{"cell_type":"markdown","metadata":{"id":"1dn246hMO3WT"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ermx29zttBxz"},"outputs":[],"source":["import collections\n","import dp_accounting\n","import numpy as np\n","import pandas as pd\n","import csv\n","import tensorflow as tf\n","import tensorflow_federated as tff\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"FmY7EwYFO_8D"},"source":["## Define global variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWNmE_XiYcMR"},"outputs":[],"source":["TOTAL_CLIENTS = 8\n","SEED = 42\n","TARGET_DELTA = 1e-4\n","EXP_NO = 10\n","\n","path = '/path/to/output'"]},{"cell_type":"markdown","metadata":{"id":"Pj9xF8h6PKIi"},"source":["## Data preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Kota72ftZTx"},"outputs":[],"source":["def load_diabetes_dataset():\n","  # load data\n","  data_dir = '/path/to/diabetes.csv'\n","  data = pd.read_csv(data_dir)\n","  data = data.sample(frac=1).reset_index(drop=True)\n","\n","  # handle null values\n","  data['BMI'] = data.BMI.mask(data.BMI == 0, (data['BMI'].mean(skipna=True)))\n","  data['BloodPressure'] = data.BloodPressure.mask(data.BloodPressure == 0, (data['BloodPressure'].mean(skipna=True)))\n","  data['Glucose'] = data.Glucose.mask(data.Glucose == 0, (data['Glucose'].mean(skipna=True)))\n","  data['SkinThickness'] = data.Glucose.mask(data.Glucose == 0, (data['SkinThickness'].mean(skipna=True)))\n","  data['Insulin'] = data.Glucose.mask(data.Glucose == 0, (data['Insulin'].mean(skipna=True)))\n","\n","  cols_to_norm = ['Pregnancies', 'Glucose','BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n","  data_normalized = data[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n","\n","  min_dict = {}\n","  max_dict = {}\n","\n","  for col in ['Pregnancies', 'BMI', 'Age']:\n","    min_dict[col] = data[col].min()\n","    max_dict[col] = data[col].max()\n","\n","  data_normalized['Outcome'] = data['Outcome']\n","  data = data_normalized\n","\n","  # train test split\n","  train_data, test_data = train_test_split(data, test_size=0.2, random_state=SEED)\n","  train_samples = len(train_data)\n","  num_clients = TOTAL_CLIENTS\n","\n","  # create tf datasets\n","  client_train_dataframes = np.array_split(train_data, num_clients)\n","\n","  def dataframe_to_tf_dataset(df):\n","    features = df.drop(['Outcome'], axis=1).values\n","    labels = df['Outcome'].values\n","    return tf.data.Dataset.from_tensor_slices((features, labels))#.batch(len(df))\n","\n","  client_train_datasets = [dataframe_to_tf_dataset(client_df).batch(len(client_df)) for client_df in client_train_dataframes]\n","  test_data = dataframe_to_tf_dataset(test_data)\n","\n","  def create_tf_dataset_for_client_fn(client_id):\n","    return client_train_datasets[client_id]\n","\n","  federated_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n","      client_ids=range(num_clients),\n","      serializable_dataset_fn=create_tf_dataset_for_client_fn\n","  )\n","\n","  federated_train_data = [\n","    federated_data.create_tf_dataset_for_client(client_id)\n","    for client_id in federated_data.client_ids\n","]\n","\n","  return federated_data, test_data, min_dict, max_dict, train_samples\n","\n","train_data, test_data, min_dict, max_dict, train_samples = load_diabetes_dataset()"]},{"cell_type":"markdown","metadata":{"id":"c99b3LrrOg-j"},"source":["### Test data splitting for fairness evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABzRLCkAbTX6"},"outputs":[],"source":["def split_test_data():\n","  # groupings\n","  pregnancies_grouping = [(x - min_dict['Pregnancies']) / (max_dict['Pregnancies'] - min_dict['Pregnancies']) for x in[0,3,6,9]]\n","  bmi_grouping = [(x - min_dict['BMI']) / (max_dict['BMI'] - min_dict['BMI']) for x in [18.5, 25, 30, 40]]\n","  age_grouping = [(x - min_dict['Age']) / (max_dict['Age'] - min_dict['Age']) for x in [30,40,50]]\n","\n","\n","  # pregnancies\n","  test_data_preg_1 = test_data.filter(lambda x, y: tf.equal(x[0], pregnancies_grouping[0])).batch(len(test_data))\n","  test_data_preg_2 = test_data.filter(lambda x, y: tf.logical_and(tf.greater(x[0], pregnancies_grouping[0]), tf.less(x[0], pregnancies_grouping[1]))).batch(len(test_data))\n","  test_data_preg_3 = test_data.filter(lambda x, y: tf.logical_and(tf.greater_equal(x[0], pregnancies_grouping[1]), tf.less(x[0], pregnancies_grouping[2]))).batch(len(test_data))\n","  test_data_preg_4 = test_data.filter(lambda x, y: tf.logical_and(tf.greater_equal(x[0], pregnancies_grouping[2]), tf.less(x[0], pregnancies_grouping[3]))).batch(len(test_data))\n","  test_data_preg_5 = test_data.filter(lambda x, y: tf.greater_equal(x[0], pregnancies_grouping[3])).batch(len(test_data))\n","\n","  test_data_preg = {'preg_=0': test_data_preg_1, 'preg_1-2': test_data_preg_2, 'preg_3-5': test_data_preg_3, 'preg_6-8': test_data_preg_4, 'preg_>=9': test_data_preg_5}\n","\n","  # bmi\n","  test_data_bmi_1 = test_data.filter(lambda x, y: tf.less_equal(x[5], bmi_grouping[0])).batch(len(test_data))\n","  test_data_bmi_2 = test_data.filter(lambda x, y: tf.logical_and(tf.greater(x[5], bmi_grouping[0]), tf.less_equal(x[5], bmi_grouping[1]))).batch(len(test_data))\n","  test_data_bmi_3 = test_data.filter(lambda x, y: tf.logical_and(tf.greater(x[5], bmi_grouping[1]), tf.less_equal(x[5], bmi_grouping[2]))).batch(len(test_data))\n","  test_data_bmi_4 = test_data.filter(lambda x, y: tf.logical_and(tf.greater(x[5], bmi_grouping[2]), tf.less_equal(x[5], bmi_grouping[3]))).batch(len(test_data))\n","  test_data_bmi_5 = test_data.filter(lambda x, y: tf.greater(x[5], bmi_grouping[3])).batch(len(test_data))\n","\n","  test_data_bmi = {'bmi_0-18.5': test_data_bmi_1, 'bmi_18.5-25': test_data_bmi_2, 'bmi_25-30': test_data_bmi_3, 'bmi_30-40': test_data_bmi_4, 'bmi_>40': test_data_bmi_5}\n","\n","  # age\n","  test_data_age_1 = test_data.filter(lambda x, y: tf.less_equal(x[7], age_grouping[0])).batch(len(test_data))\n","  test_data_age_2 = test_data.filter(lambda x, y: tf.logical_and(tf.greater(x[7], age_grouping[0]), tf.less_equal(x[7], age_grouping[1]))).batch(len(test_data))\n","  test_data_age_3 = test_data.filter(lambda x, y: tf.logical_and(tf.greater(x[7], age_grouping[1]), tf.less_equal(x[7], age_grouping[2]))).batch(len(test_data))\n","  test_data_age_4 = test_data.filter(lambda x, y: tf.greater(x[7], age_grouping[2])).batch(len(test_data))\n","\n","  test_data_age = {'age_<30': test_data_age_1, 'age_30-40': test_data_age_2, 'age_40-50': test_data_age_3, 'age_>50': test_data_age_4}\n","\n","  return test_data_preg, test_data_bmi, test_data_age\n","\n","\n","test_data_preg, test_data_bmi, test_data_age = split_test_data()\n","\n","test_data = test_data.batch(len(test_data))"]},{"cell_type":"markdown","metadata":{"id":"KlDZDeykPIV9"},"source":["## Model definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gzy4rTDK2gVU"},"outputs":[],"source":["def my_model_fn():\n","  model = tf.keras.models.Sequential([\n","      tf.keras.layers.Dense(units=200, input_shape=(8,)),\n","        tf.keras.layers.ReLU(),\n","        tf.keras.layers.Dense(200),\n","        tf.keras.layers.ReLU(),\n","        tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])\n","  return tff.learning.models.from_keras_model(\n","      keras_model=model,\n","      loss=tf.keras.losses.BinaryCrossentropy(),\n","      input_spec=test_data.element_spec,\n","      metrics=[tf.keras.metrics.BinaryAccuracy(),\n","            tf.keras.metrics.Precision(),\n","            tf.keras.metrics.Recall(),\n","            tf.keras.metrics.TruePositives(),\n","            tf.keras.metrics.TrueNegatives(),\n","            tf.keras.metrics.FalsePositives(),\n","            tf.keras.metrics.FalseNegatives(),])"]},{"cell_type":"markdown","metadata":{"id":"r-6d87lSPKR4"},"source":["## Fairness measures"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DItJAQUMcLCN"},"outputs":[],"source":["def measureFairness(mconf_p, mconf_u):\n","\n","  (TN_p, FP_p, FN_p, TP_p) = mconf_p\n","  (TN_u, FP_u, FN_u, TP_u) = mconf_u\n","\n","  def _DI_degree():\n","    # |1-(SR_u/SR_p)| --> where SR is the selection rate SR=(TP+FP)/N\n","    PR_p = FP_p + TP_p\n","    PR_u = FP_u + TP_u\n","    N_p = TN_p + FP_p + FN_p + TP_p\n","    N_u = TN_u + FP_u + FN_u + TP_u\n","\n","    SR_p = PR_p/N_p\n","    SR_u = PR_u/N_u\n","\n","    if SR_p == 0 or SR_u == 0:\n","        return 1\n","\n","    # expecting SR_p being higher...\n","    DI_degree = abs(1-SR_u/SR_p)\n","\n","    return DI_degree\n","\n","  def _EOP_difference():\n","    # |TPR_p - TPR_u|\n","\n","    TPR_p = TP_p/(TP_p + FN_p)\n","    TPR_u = TP_u/(TP_u + FN_u)\n","\n","    EOP_difference = abs(TPR_p - TPR_u)\n","\n","    return EOP_difference\n","\n","  def _EODD_difference():\n","    # 0.5 * (|TPR_p - TPR_u| + |TNR_p - TNR_u|)\n","\n","    TPR_p = TP_p/(TP_p + FN_p)\n","    TPR_u = TP_u/(TP_u + FN_u)\n","\n","    TNR_p = TN_p/(TN_p + FP_p)\n","    TNR_u = TN_u/(TN_u + FP_u)\n","\n","    EODD_difference = 0.5 * (abs(TPR_p - TPR_u) + abs(TNR_p - TNR_u))\n","\n","    return EODD_difference\n","\n","  def _SP_difference():\n","    # |((TP_p + FP_p)/N_p) - ((TP_u + FP_u)/N_u)|\n","\n","    N_p = TN_p + FP_p + FN_p + TP_p\n","    N_u = TN_u + FP_u + FN_u + TP_u\n","\n","    SP_difference = abs(((TP_p + FP_p)/N_p) - ((TP_u + FP_u)/N_u))\n","\n","    return SP_difference\n","\n","  DI_degree = round(_DI_degree(), 4)\n","  EOP_difference = round(_EOP_difference(), 4)\n","  EODD_difference = round(_EODD_difference(), 4)\n","  SP_difference = round(_SP_difference(), 4)\n","\n","  print(\"Disparate Impact: \", DI_degree)\n","  print(\"EOP difference: \", EOP_difference)\n","  print(\"EODD difference: \", EODD_difference)\n","  print(\"SP difference: \", SP_difference)\n","\n","  return {\"DI_degree\": DI_degree, \"EOP_difference\": EOP_difference, \"EODD_difference\": EODD_difference, \"SP_difference\": SP_difference}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xtyyD22l13cX"},"outputs":[],"source":["def evaluate_fairness(eval_process, eval_state, model_weights, noise_multiplier=0.0):\n","  def _createConfMatrix(data_frame):\n","    results = data_frame.iloc[-1]\n","    tn = results['true_negatives']\n","    fp = results['false_positives']\n","    fn = results['false_negatives']\n","    tp = results['true_positives']\n","    return (tn, fp, fn, tp)\n","\n","  def _eval_to_mconf(ds):\n","    new_eval_state = eval_process.set_model_weights(eval_state, model_weights)\n","    eval_output = eval_process.next(new_eval_state, [ds])\n","    metrics = eval_output.metrics['client_work']['eval']['current_round_metrics']\n","    df = pd.DataFrame({'Round': round, 'NoiseMultiplier': noise_multiplier, **metrics}, index=[0])\n","    return _createConfMatrix(df)\n","\n","  def _get_all_mconf(input_dict):\n","    output_dict = {}\n","    for group, ds in input_dict.items():\n","      output_dict[group] = _eval_to_mconf(ds)\n","\n","    return output_dict\n","\n","  def _measure_fairness_for_all_constellations(mconf_dict):\n","    for key, current_tuple in mconf_dict.items():\n","      mconf_u = current_tuple\n","\n","      other_tuples = [v for k, v in mconf_dict.items() if k != key]\n","      mconf_p = tuple(sum(x) for x in zip(*other_tuples))\n","\n","      print('group: ', key)\n","      results = measureFairness(mconf_p, mconf_u)\n","      results['noise_multiplier'] = noise_multiplier\n","\n","      append_dict_to_csv(results, results_path + key + '.csv')\n","\n","  def append_dict_to_csv(input_dict, file_path):\n","\n","    with open(file_path, mode='a', newline='') as file:\n","        writer = csv.writer(file)\n","\n","        if file.tell() == 0:\n","            writer.writerow(input_dict.keys())\n","\n","        writer.writerow(input_dict.values())\n","\n","  for test_data in [test_data_preg, test_data_bmi, test_data_age]:\n","    mconf_dict = _get_all_mconf(test_data)\n","    print('measuring fairness on ' + str(test_data))\n","    _measure_fairness_for_all_constellations(mconf_dict)\n"]},{"cell_type":"markdown","source":["## Baseline training"],"metadata":{"id":"Vb3FwI4LAfNn"}},{"cell_type":"code","source":["def train_baseline(rounds, data_frame):\n","\n","  learning_process = tff.learning.algorithms.build_unweighted_fed_avg(\n","        my_model_fn,\n","        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1),\n","        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0, momentum=0.9))\n","\n","  eval_process = tff.learning.algorithms.build_fed_eval(my_model_fn)\n","  state = learning_process.initialize()\n","  eval_state = eval_process.initialize()\n","\n","  all_clients = train_data.client_ids\n","  all_train_data = [\n","        train_data.create_tf_dataset_for_client(client)\n","        for client in all_clients\n","    ]\n","\n","  for round in range(rounds):\n","    if round % 5 == 0:\n","      model_weights = learning_process.get_model_weights(state)\n","      eval_state = eval_process.set_model_weights(eval_state, model_weights)\n","      eval_output = eval_process.next(eval_state, [test_data])\n","      metrics = eval_output.metrics['client_work']['eval']['current_round_metrics']\n","      if round < 25 or round % 25 == 0:\n","        print(f'Round {round:3d}: {metrics}')\n","      data_frame = pd.concat([data_frame, pd.DataFrame({'round': round,\n","                                      **metrics}, index=[0])], ignore_index=True)\n","\n","    # model update\n","    result = learning_process.next(state, all_train_data)\n","    state = result.state\n","    metrics = result.metrics\n","\n","\n","  model_weights = learning_process.get_model_weights(state)\n","  eval_state = eval_process.set_model_weights(eval_state, model_weights)\n","  eval_output = eval_process.next(eval_state, [test_data])\n","  metrics = eval_output.metrics['client_work']['eval']['current_round_metrics']\n","  print(f'Round {rounds:3d}: {metrics}')\n","  data_frame = pd.concat([data_frame, pd.DataFrame({'round': round,\n","                                      **metrics}, index=[0])], ignore_index=True)\n","\n","  evaluate_fairness(eval_process, eval_state, model_weights)\n","\n","  return data_frame"],"metadata":{"id":"AV2HQJryAhSd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame()\n","rounds = 200\n","results_path = path + 'baseline/'\n","\n","for exp in range(EXP_NO):\n","  print(f'Starting training with experiment: {exp}')\n","  df = train_baseline(rounds, df)\n","\n","# add f1 score\n","df['f1_score'] = 2 * (df['precision'] * df['recall']) / (df['precision'] + df['recall'])\n","\n","# save results\n","df.to_csv(results_path + 'performance_complete.csv', index=False)"],"metadata":{"id":"t3N41oSMAlUQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3G9VdGyPPgeV"},"source":["## DP Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avqaQu1LTd3C"},"outputs":[],"source":["def train(rounds, noise_multiplier, data_frame, samples_per_round=train_samples):\n","  accountant = dp_accounting.rdp.RdpAccountant()\n","\n","  aggregation_factory = tff.learning.model_update_aggregator.dp_aggregator(\n","      noise_multiplier, samples_per_round)\n","\n","  learning_process = tff.learning.algorithms.build_unweighted_fed_avg(\n","        my_model_fn,\n","        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1),\n","        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0, momentum=0.9),\n","        model_aggregator=aggregation_factory)\n","\n","  eval_process = tff.learning.algorithms.build_fed_eval(my_model_fn)\n","\n","  state = learning_process.initialize()\n","  eval_state = eval_process.initialize()\n","\n","  all_clients = train_data.client_ids\n","  all_train_data = [\n","        train_data.create_tf_dataset_for_client(client)\n","        for client in all_clients\n","    ]\n","\n","  sampling_probability = samples_per_round / TOTAL_CLIENTS\n","\n","  for round in range(rounds):\n","    if round % 5 == 0:\n","      model_weights = learning_process.get_model_weights(state)\n","      eval_state = eval_process.set_model_weights(eval_state, model_weights)\n","      eval_output = eval_process.next(eval_state, [test_data])\n","      metrics = eval_output.metrics['client_work']['eval']['current_round_metrics']\n","      if round < 25 or round % 25 == 0:\n","        print(f'Round {round:3d}: {metrics}')\n","      data_frame = pd.concat([data_frame, pd.DataFrame({'round': round,\n","                                      'noise_multiplier': noise_multiplier,\n","                                      **metrics}, index=[0])], ignore_index=True)\n","\n","    # model update\n","    result = learning_process.next(state, all_train_data)\n","    state = result.state\n","    metrics = result.metrics\n","\n","    accountant.compose(dp_accounting.GaussianDpEvent(noise_multiplier))\n","\n","  epsilon = accountant.get_epsilon(TARGET_DELTA)\n","  print(f\"Total privacy budget (epsilon): {epsilon}\")\n","\n","  model_weights = learning_process.get_model_weights(state)\n","  eval_state = eval_process.set_model_weights(eval_state, model_weights)\n","  eval_output = eval_process.next(eval_state, [test_data])\n","  metrics = eval_output.metrics['client_work']['eval']['current_round_metrics']\n","  print(f'Round {rounds:3d}: {metrics}')\n","  print(f'Privacy budget (epsilon) spent: {epsilon:.4f}')\n","\n","  data_frame = pd.concat([data_frame, pd.DataFrame({'round': round,\n","                                      'noise_multiplier': noise_multiplier, 'epsilon': epsilon,\n","                                      **metrics}, index=[0])], ignore_index=True)\n","\n","  evaluate_fairness(eval_process, eval_state, model_weights, noise_multiplier)\n","\n","  return data_frame"]},{"cell_type":"code","source":["df = pd.DataFrame()\n","rounds = 200\n","noise_multipliers = [1.0, 5.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0]\n","results_path = path + 'dp/'\n","\n","for exp in range(EXP_NO):\n","  print(f'Starting training with experiment: {exp}')\n","  for noise_multiplier in noise_multipliers:\n","    print(f'Starting training with noise multiplier: {noise_multiplier}')\n","    df = train(rounds, noise_multiplier, df)\n","    print()\n","\n","# add f1 score\n","df['f1_score'] = 2 * (df['precision'] * df['recall']) / (df['precision'] + df['recall'])\n","\n","# save results\n","df.to_csv(results_path + 'performance_complete.csv', index=False)\n","# save just last round's results\n","filtered_df = df[df['round'] == df['round'].max()]\n","filtered_df.to_csv(results_path + 'performance.csv', index=False)"],"metadata":{"id":"EzOC8ci39x0t"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1U42slF2gEe3nseAUSzsaXd8Xos8aJOgC","timestamp":1723043635768}],"authorship_tag":"ABX9TyN5jknovaTFfkAi5Ngxn+6+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}