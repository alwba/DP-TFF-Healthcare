{"cells":[{"cell_type":"markdown","metadata":{"id":"RuN185vVMwjo"},"source":["# Notebook for federated training on heart disease dataset"]},{"cell_type":"markdown","metadata":{"id":"RBp5s3z_Mtix"},"source":["## Installation of required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znPpaE8EtAHx"},"outputs":[],"source":["!pip install --quiet --upgrade dp-accounting==0.4.3\n","!pip install --quiet --upgrade tensorflow-federated==0.84.0"]},{"cell_type":"markdown","metadata":{"id":"Wq3JQtobMnKy"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ermx29zttBxz"},"outputs":[],"source":["import collections\n","import dp_accounting\n","import numpy as np\n","import pandas as pd\n","import csv\n","import tensorflow as tf\n","import tensorflow_federated as tff\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"nG1OxDjRM_Db"},"source":["## Define global variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fntPupqBXmcn"},"outputs":[],"source":["TOTAL_CLIENTS = 8\n","SEED = 42\n","TARGET_DELTA = 1e-4\n","EXP_NO = 10\n","\n","path = '/path/to/output'"]},{"cell_type":"markdown","metadata":{"id":"teJc-rgTNGh-"},"source":["## Data preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Kota72ftZTx"},"outputs":[],"source":["def load_heart_dataset():\n","  data_dir = '/path/to/heart.csv'\n","  data = pd.read_csv(data_dir)\n","  data = data.sample(frac=1).reset_index(drop=True)\n","\n","  # mappings\n","  sex_mapping = {'M': 1, 'F': 0}\n","  cp_mapping = {'TA': 0, 'ATA': 1, 'NAP': 2, 'ASY': 3}\n","  restecg_mapping = {'Normal': 0, 'ST': 1, 'LVH': 2}\n","  exang_mapping = {'Y': 1, 'N':0}\n","  slope_mapping = {'Up': 0, 'Flat': 1, 'Down': 2}\n","\n","  data['Sex'] = data['Sex'].map(sex_mapping)\n","  data['ChestPainType'] = data['ChestPainType'].map(cp_mapping)\n","  data['RestingECG'] = data['RestingECG'].map(restecg_mapping)\n","  data['ExerciseAngina'] = data['ExerciseAngina'].map(exang_mapping)\n","  data['ST_Slope'] = data['ST_Slope'].map(slope_mapping)\n","\n","  cols_to_norm = ['Age', 'RestingBP','Cholesterol', 'MaxHR']\n","\n","  data_normalized = data[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n","  min_age = data['Age'].min()\n","  max_age = data['Age'].max()\n","\n","  data_normalized['Sex'] = data['Sex']\n","  data_normalized['ChestPainType'] = data['ChestPainType']\n","  data_normalized['FastingBS'] = data['FastingBS']\n","  data_normalized['RestingECG'] = data['RestingECG']\n","  data_normalized['ExerciseAngina'] = data['ExerciseAngina']\n","  data_normalized['Oldpeak'] = data['Oldpeak']\n","  data_normalized['ST_Slope'] = data['ST_Slope']\n","  data_normalized['Outcome'] = data['Outcome']\n","  data = data_normalized\n","\n","  # train test split\n","  train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n","\n","  train_samples = len(train_data)\n","\n","  num_clients = TOTAL_CLIENTS\n","  client_train_dataframes = np.array_split(train_data, num_clients)\n","\n","  def dataframe_to_tf_dataset(df):\n","    features = df.drop(['Outcome'], axis=1).values\n","    labels = df['Outcome'].values\n","    return tf.data.Dataset.from_tensor_slices((features, labels))#.batch(len(df))\n","\n","  client_train_datasets = [dataframe_to_tf_dataset(client_df).batch(len(client_df)) for client_df in client_train_dataframes]\n","  test_data = dataframe_to_tf_dataset(test_data)\n","\n","  def create_tf_dataset_for_client_fn(client_id):\n","    return client_train_datasets[client_id]\n","\n","  federated_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n","      client_ids=range(num_clients),\n","      serializable_dataset_fn=create_tf_dataset_for_client_fn\n","  )\n","\n","  federated_train_data = [\n","    federated_data.create_tf_dataset_for_client(client_id)\n","    for client_id in federated_data.client_ids\n","]\n","\n","  return federated_data, test_data, min_age, max_age, train_samples\n","\n","train_data, test_data, min_age, max_age, train_samples = load_heart_dataset()"]},{"cell_type":"markdown","metadata":{"id":"VRqibHnrOHJ4"},"source":["### Test data splitting for fairness evaluation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6as_RHvaBhY"},"outputs":[],"source":["def split_test_data():\n","  # sex\n","  test_data_m = test_data.filter(lambda x, y: tf.equal(x[4], 1)).batch(len(test_data))\n","  test_data_f = test_data.filter(lambda x, y: tf.equal(x[4], 0)).batch(len(test_data))\n","\n","  test_data_sex = {'sex_m': test_data_m, 'sex_f': test_data_f}\n","\n","  # age\n","  age_grouping = [(x - min_age) / (max_age - min_age) for x in [40,55,70]]\n","\n","  test_data_age_1 = test_data.filter(lambda x, y: tf.less_equal(x[0], age_grouping[0])).batch(len(test_data))\n","  test_data_age_2 = test_data.filter(lambda x, y: tf.logical_and(tf.greater(x[0], age_grouping[0]), tf.less_equal(x[0], age_grouping[1]))).batch(len(test_data))\n","  test_data_age_3 = test_data.filter(lambda x, y: tf.logical_and(tf.greater(x[0], age_grouping[1]), tf.less_equal(x[0], age_grouping[2]))).batch(len(test_data))\n","  test_data_age_4 = test_data.filter(lambda x, y: tf.greater(x[0], age_grouping[2])).batch(len(test_data))\n","\n","  test_data_age = {'age_<40': test_data_age_1, 'age_40-55': test_data_age_2, 'age_55-70': test_data_age_3, 'age_>70': test_data_age_4}\n","\n","  return test_data_sex, test_data_age\n","\n","test_data_sex, test_data_age = split_test_data()\n","\n","test_data = test_data.batch(len(test_data))"]},{"cell_type":"markdown","metadata":{"id":"SDN-wcEdOUO9"},"source":["## Model definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gzy4rTDK2gVU"},"outputs":[],"source":["def my_model_fn():\n","  model = tf.keras.models.Sequential([\n","      tf.keras.layers.Dense(units=32, input_shape=(11,)),\n","      tf.keras.layers.ReLU(),\n","        tf.keras.layers.Dense(16),\n","        tf.keras.layers.ReLU(),\n","      tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])\n","  return tff.learning.models.from_keras_model(\n","      keras_model=model,\n","      loss=tf.keras.losses.BinaryCrossentropy(),\n","      input_spec=test_data.element_spec,\n","      metrics=[tf.keras.metrics.BinaryAccuracy(),\n","            tf.keras.metrics.Precision(),\n","            tf.keras.metrics.Recall(),\n","            tf.keras.metrics.TruePositives(),\n","            tf.keras.metrics.TrueNegatives(),\n","            tf.keras.metrics.FalsePositives(),\n","            tf.keras.metrics.FalseNegatives(),])"]},{"cell_type":"markdown","metadata":{"id":"QKBbopvvOXCd"},"source":["## Fairness measures"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3R1ThyQpcNCv"},"outputs":[],"source":["def measureFairness(mconf_p, mconf_u):\n","\n","  (TN_p, FP_p, FN_p, TP_p) = mconf_p\n","  (TN_u, FP_u, FN_u, TP_u) = mconf_u\n","\n","  def _DI_degree():\n","    # |1-(SR_u/SR_p)| --> where SR is the selection rate SR=(TP+FP)/N\n","    PR_p = FP_p + TP_p\n","    PR_u = FP_u + TP_u\n","    N_p = TN_p + FP_p + FN_p + TP_p\n","    N_u = TN_u + FP_u + FN_u + TP_u\n","\n","    SR_p = PR_p/N_p\n","    SR_u = PR_u/N_u\n","\n","    if SR_p == 0 or SR_u == 0:\n","        return 1\n","\n","    # expecting SR_p being higher...\n","    DI_degree = abs(1-SR_u/SR_p)\n","\n","    return DI_degree\n","\n","  def _EOP_difference():\n","    # |TPR_p - TPR_u|\n","\n","    TPR_p = TP_p/(TP_p + FN_p)\n","    TPR_u = TP_u/(TP_u + FN_u)\n","\n","    EOP_difference = abs(TPR_p - TPR_u)\n","\n","    return EOP_difference\n","\n","  def _EODD_difference():\n","    # 0.5 * (|TPR_p - TPR_u| + |TNR_p - TNR_u|)\n","\n","    TPR_p = TP_p/(TP_p + FN_p)\n","    TPR_u = TP_u/(TP_u + FN_u)\n","\n","    TNR_p = TN_p/(TN_p + FP_p)\n","    TNR_u = TN_u/(TN_u + FP_u)\n","\n","    EODD_difference = 0.5 * (abs(TPR_p - TPR_u) + abs(TNR_p - TNR_u))\n","\n","    return EODD_difference\n","\n","  def _SP_difference():\n","    # |((TP_p + FP_p)/N_p) - ((TP_u + FP_u)/N_u)|\n","\n","    N_p = TN_p + FP_p + FN_p + TP_p\n","    N_u = TN_u + FP_u + FN_u + TP_u\n","\n","    SP_difference = abs(((TP_p + FP_p)/N_p) - ((TP_u + FP_u)/N_u))\n","\n","    return SP_difference\n","\n","  DI_degree = round(_DI_degree(), 4)\n","  EOP_difference = round(_EOP_difference(), 4)\n","  EODD_difference = round(_EODD_difference(), 4)\n","  SP_difference = round(_SP_difference(), 4)\n","\n","  print(\"Disparate Impact: \", DI_degree)\n","  print(\"EOP difference: \", EOP_difference)\n","  print(\"EODD difference: \", EODD_difference)\n","  print(\"SP difference: \", SP_difference)\n","\n","  return {\"DI_degree\": DI_degree, \"EOP_difference\": EOP_difference, \"EODD_difference\": EODD_difference, \"SP_difference\": SP_difference}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LkssX_XChKA"},"outputs":[],"source":["def evaluate_fairness(eval_process, eval_state, model_weights, noise_multiplier=0.0):\n","\n","  def _create_mconf(data_frame):\n","    results = data_frame.iloc[-1]\n","    tn = results['true_negatives']\n","    fp = results['false_positives']\n","    fn = results['false_negatives']\n","    tp = results['true_positives']\n","    return (tn, fp, fn, tp)\n","\n","  def _eval_to_mconf(ds):\n","    new_eval_state = eval_process.set_model_weights(eval_state, model_weights)\n","    eval_output = eval_process.next(new_eval_state, [ds])\n","    metrics = eval_output.metrics['client_work']['eval']['current_round_metrics']\n","    df = pd.DataFrame({'Round': round, 'NoiseMultiplier': noise_multiplier, **metrics}, index=[0])\n","    return _create_mconf(df)\n","\n","  def _get_all_mconf(input_dict):\n","    output_dict = {}\n","    for group, ds in input_dict.items():\n","      output_dict[group] = _eval_to_mconf(ds)\n","\n","    return output_dict\n","\n","  def _measure_fairness_for_all_constellations(mconf_dict):\n","\n","    for key, current_tuple in mconf_dict.items():\n","      mconf_u = current_tuple\n","\n","      other_tuples = [v for k, v in mconf_dict.items() if k != key]\n","      mconf_p = tuple(sum(x) for x in zip(*other_tuples))\n","\n","      print('group: ', key)\n","      results = measureFairness(mconf_p, mconf_u)\n","      results['noise_multiplier'] = noise_multiplier\n","\n","      append_dict_to_csv(results, results_path + key + '.csv')\n","\n","  def append_dict_to_csv(input_dict, file_path):\n","\n","    with open(file_path, mode='a', newline='') as file:\n","        writer = csv.writer(file)\n","\n","        if file.tell() == 0:\n","            writer.writerow(input_dict.keys())\n","\n","        writer.writerow(input_dict.values())\n","\n","\n","  for test_data in [test_data_sex, test_data_age]:\n","    mconf_dict = _get_all_mconf(test_data)\n","    print('measuring fairness on ' + str(test_data))\n","    _measure_fairness_for_all_constellations(mconf_dict)"]},{"cell_type":"markdown","metadata":{"id":"Zp_lmMznx1Bj"},"source":["## Baseline training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtifV0vsx377"},"outputs":[],"source":["def train_baseline(rounds, data_frame):\n","\n","  learning_process = tff.learning.algorithms.build_unweighted_fed_avg(\n","        my_model_fn,\n","        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1),\n","        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0, momentum=0.9))\n","\n","  eval_process = tff.learning.algorithms.build_fed_eval(my_model_fn)\n","  state = learning_process.initialize()\n","  eval_state = eval_process.initialize()\n","\n","  all_clients = train_data.client_ids\n","  all_train_data = [\n","        train_data.create_tf_dataset_for_client(client)\n","        for client in all_clients\n","    ]\n","\n","  for round in range(rounds):\n","    if round % 5 == 0:\n","      model_weights = learning_process.get_model_weights(state)\n","      eval_state = eval_process.set_model_weights(eval_state, model_weights)\n","      eval_output = eval_process.next(eval_state, [test_data])\n","      metrics = eval_output.metrics['client_work']['eval']['current_round_metrics']\n","      if round < 25 or round % 25 == 0:\n","        print(f'Round {round:3d}: {metrics}')\n","      data_frame = pd.concat([data_frame, pd.DataFrame({'round': round,\n","                                      **metrics}, index=[0])], ignore_index=True)\n","\n","    # model update\n","    result = learning_process.next(state, all_train_data)\n","    state = result.state\n","    metrics = result.metrics\n","\n","\n","  model_weights = learning_process.get_model_weights(state)\n","  eval_state = eval_process.set_model_weights(eval_state, model_weights)\n","  eval_output = eval_process.next(eval_state, [test_data])\n","  metrics = eval_output.metrics['client_work']['eval']['current_round_metrics']\n","  print(f'Round {rounds:3d}: {metrics}')\n","  data_frame = pd.concat([data_frame, pd.DataFrame({'round': round,\n","                                      **metrics}, index=[0])], ignore_index=True)\n","\n","  evaluate_fairness(eval_process, eval_state, model_weights)\n","\n","  return data_frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ObquxfsNz4c6"},"outputs":[],"source":["df = pd.DataFrame()\n","rounds = 200\n","results_path = path + 'baseline/'\n","\n","for exp in range(EXP_NO):\n","  print(f'Starting training with experiment: {exp}')\n","  df = train_baseline(rounds, df)\n","\n","# add f1 score\n","df['f1_score'] = 2 * (df['precision'] * df['recall']) / (df['precision'] + df['recall'])\n","\n","# save results\n","df.to_csv(results_path + 'performance_complete.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"SvbLRKyGPpoI"},"source":["## DP Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avqaQu1LTd3C"},"outputs":[],"source":["def train(rounds, noise_multiplier, data_frame, samples_per_round=train_samples):\n","  accountant = dp_accounting.rdp.RdpAccountant()\n","\n","  aggregation_factory = tff.learning.model_update_aggregator.dp_aggregator(\n","      noise_multiplier, samples_per_round)\n","\n","  learning_process = tff.learning.algorithms.build_unweighted_fed_avg(\n","        my_model_fn,\n","        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1),\n","        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0, momentum=0.9),\n","        model_aggregator=aggregation_factory)\n","\n","  eval_process = tff.learning.algorithms.build_fed_eval(my_model_fn)\n","\n","  state = learning_process.initialize()\n","  eval_state = eval_process.initialize()\n","\n","  all_clients = train_data.client_ids\n","  all_train_data = [\n","        train_data.create_tf_dataset_for_client(client)\n","        for client in all_clients\n","    ]\n","\n","  sampling_probability = samples_per_round / TOTAL_CLIENTS\n","\n","  for round in range(rounds):\n","    if round % 5 == 0:\n","      model_weights = learning_process.get_model_weights(state)\n","      eval_state = eval_process.set_model_weights(eval_state, model_weights)\n","      eval_output = eval_process.next(eval_state, [test_data])\n","      metrics = eval_output.metrics['client_work']['eval']['current_round_metrics']\n","      if round < 25 or round % 25 == 0:\n","        print(f'Round {round:3d}: {metrics}')\n","      data_frame = pd.concat([data_frame, pd.DataFrame({'round': round,\n","                                      'noise_multiplier': noise_multiplier,\n","                                      **metrics}, index=[0])], ignore_index=True)\n","\n","    # model update\n","    result = learning_process.next(state, all_train_data)\n","    state = result.state\n","    metrics = result.metrics\n","\n","    accountant.compose(dp_accounting.GaussianDpEvent(noise_multiplier))\n","\n","  epsilon = accountant.get_epsilon(TARGET_DELTA)\n","  print(f\"Total privacy budget (epsilon): {epsilon}\")\n","\n","  model_weights = learning_process.get_model_weights(state)\n","  eval_state = eval_process.set_model_weights(eval_state, model_weights)\n","  eval_output = eval_process.next(eval_state, [test_data])\n","  metrics = eval_output.metrics['client_work']['eval']['current_round_metrics']\n","  print(f'Round {rounds:3d}: {metrics}')\n","  print(f'Privacy budget (epsilon) spent: {epsilon:.4f}')\n","\n","  data_frame = pd.concat([data_frame, pd.DataFrame({'round': round,\n","                                      'noise_multiplier': noise_multiplier, 'epsilon': epsilon,\n","                                      **metrics}, index=[0])], ignore_index=True)\n","\n","  evaluate_fairness(eval_process, eval_state, model_weights, noise_multiplier)\n","\n","  return data_frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvRzECCKCHTS"},"outputs":[],"source":["df = pd.DataFrame()\n","rounds = 200\n","noise_multipliers = [1.0, 5.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0]\n","results_path = path + 'dp/'\n","\n","for exp in range(EXP_NO):\n","  print(f'Starting training with experiment: {exp}')\n","  for noise_multiplier in noise_multipliers:\n","    print(f'Starting training with noise multiplier: {noise_multiplier}')\n","    df = train(rounds, noise_multiplier, df)\n","    print()\n","\n","# add f1 score\n","df['f1_score'] = 2 * (df['precision'] * df['recall']) / (df['precision'] + df['recall'])\n","\n","# save results\n","df.to_csv(results_path + 'performance_complete.csv', index=False)\n","# save just last round's results\n","filtered_df = df[df['round'] == df['round'].max()]\n","filtered_df.to_csv(results_path + 'performance.csv', index=False)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP4ss0kcsvMdzzxeXNY+ith"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}